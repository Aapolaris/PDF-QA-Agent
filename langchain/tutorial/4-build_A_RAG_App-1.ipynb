{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3684b2fe",
   "metadata": {},
   "source": [
    "# 制作一个RAG应用-1\n",
    "\n",
    "[build a RAG app | part 1](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "**简介：**<br>\n",
    "**本节介绍了以下内容：**\n",
    "- 逐步骤得介绍了一个简单RAG应用的搭建方法。该应用是针对网页内容进行检索。且仅支持单轮对话、单轮检索\n",
    "- 本节利用 Chains 实现，其中指出 `retrieve` 在本节中是流程的一部分，而不是作为一个 “工具”。Chains链为: `_start_ --> retrieve --> generate` \n",
    "- 在本节最后提出增加一个 `query_analysis` 对用户输入问题进行重写（/提纯/增强...），使得检索能够更加高效。即使得 `_start_ --> query_analysis --> retrieve --> generate`\n",
    "\n",
    "> 这里只关注对于非结构数据的QA，结构数据的QA参考 [Build a QA system over sql data](https://python.langchain.com/docs/tutorials/sql_qa/)\n",
    "\n",
    "### 一个经典得RAG应用通常包含以下两个部分：\n",
    "\n",
    "#### 1. Inxdexing\n",
    "从数据源中获取数据并index。这通常在离线情况下发生。又包含以下步骤：\n",
    "1. Load： 使用document loader加载数据\n",
    "2. spkit：使用text spliter将文本分为更小的chunk，以便于index和输入到模型中。（模型输入往往有最大token限制）\n",
    "3. store：需要一些地方存储和index我们的chunks。这通常使用VectorStore和Embeding模型完成\n",
    "\n",
    "#### 2. Retrieval and generation\n",
    "实际的 RAG 链在运行时接受用户查询，并从索引中检索相关数据，然后将其传递给模型。<br>\n",
    "1. Retrieve: 给定用户输入，使用 `Retriever` 从storage中检索出与用户数输入有关得片段\n",
    "2. Generate: 使用一个包含用户输入与检索数据得prompt从CahtModel或LLM中得到回答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a3a36",
   "metadata": {},
   "source": [
    "## 加载重要组件\n",
    "\n",
    "语言模型、嵌入模型、向量存储库<br>\n",
    "这里语言模型和嵌入模型都选的Gemini模型，向量数据库使用得是FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eb2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ.get('GOOGLE_API_KEY')\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider='google-genai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faad9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/gemini-embedding-001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c49a72",
   "metadata": {},
   "source": [
    "文本 --> 向量 --> FAISS索引 --> 检索 --> 文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d907bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query('hellow, world!'))\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # 创建FAISS索引，用于存储和检索向量，L2表示用欧几里得距离来衡量向量相似度\n",
    "\n",
    "# 用FAISS封装类，把FAISS检索和文档存储结合起来\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(), # 用于存储原始文档内容。由于FAISS只存向量不存文本，因此需要一个docstore来做向量-文本得映射\n",
    "    index_to_docstore_id={}  # 记录FAISS索引位置->文档ID得映射关系\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ae604",
   "metadata": {},
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title', 'post-header')  # 只保留网页的title、headers、content\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {'context': retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state['context'])\n",
    "    messages = prompt.invoke({'question': state['question'], 'context': docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {'answer': response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph = graph_builder.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed61033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller, simpler, and more manageable steps. This process enhances model performance on difficult tasks by allowing models to \"think step by step.\" It transforms large problems into multiple, manageable sub-tasks.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({'question': 'what is task decomposition?'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cadc6",
   "metadata": {},
   "source": [
    "## 1. Indexing\n",
    "\n",
    "包括document_loaders, embedings, and vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf085fa",
   "metadata": {},
   "source": [
    "### 1）Loading documents\n",
    "\n",
    "DocumenLoaders: 用于加载博客中的内容。返回的是Documents对象的列表\n",
    "\n",
    "本示例中使用`WebBaseLoader`从指定url中加载HTML，然后用`BeautifulSoup`把它解析成文本。同时使用`bs_kwargs`筛选想要的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "affaa17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "bs4_strainer =bs4.SoupStrainer(class_=('post-title', 'post-header', 'post-content'))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/', ), \n",
    "    bs_kwargs={'parse_only': bs4_strainer}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f'total characters: {len(docs[0].page_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396546a",
   "metadata": {},
   "source": [
    "### 2）spliting documents\n",
    "\n",
    "将整个文档分割为多个`chunk`以便嵌入和存储。在检索时返回最相关的片段。\n",
    "\n",
    "这里使用`RecursiveCharacterTextSplitter`，它使用一些常用的分隔符（如转行...）来递归的分割文本，直到每个chunk的大小合适为止。<br>\n",
    "这是针对一般文本使用情况推荐的文本分割器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b9ef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014091c",
   "metadata": {},
   "source": [
    "### 3）storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbed8405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['093e71b9-ed06-405f-b25d-a4b635a2d79e', '8eefee29-e61d-40f8-9c0f-f0441e437a79', '29917a41-3057-46cf-a575-7b0311983a94']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3525a0",
   "metadata": {},
   "source": [
    "## 2. Retrieval and Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e1682",
   "metadata": {},
   "source": [
    "### 提示词模板\n",
    "这里我们使用RAG的常用prompt模板，该模板被收录进[LangChain prompt hub](https://smith.langchain.com/hub/rlm/rag-prompt?_gl=1*17aaaip*_gcl_au*MTc2MjgwMjUyNy4xNzU4NjE1MzM4*_ga*NjA0MzU1NzU3LjE3NTg2MTUzMzk.*_ga_47WX3HKKY2*czE3NTg5NTgyNDIkbzE1JGcxJHQxNzU4OTU5MTExJGoxMCRsMCRoMA..&organizationId=0ff31a47-e168-4da9-9677-0b214e215a1a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b19600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (questions goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {'context': '(context goes here)', 'question': '(questions goes here)'}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960058ed",
   "metadata": {},
   "source": [
    "### 利用LangGraph实现\n",
    "接下来使用LangGraph将检索和生成步骤整合到一个应用中。\n",
    "\n",
    "> 其实不一定非要使用LangGraph，但使用LangGraph的优点很多（其它方法参考官网）\n",
    "\n",
    "使用LangGrpah，需要定义：\n",
    "1. 应用的state\n",
    "2. 应用的nodes\n",
    "3. 应用的控制流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662285aa",
   "metadata": {},
   "source": [
    "#### 1）State\n",
    "state用于控制输入到应用中的输入是什么样的，在steps中传输和应用的输出。<br>\n",
    "它通常是 `TypeDict` 或 `Pydantic BaseModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704d8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649b966",
   "metadata": {},
   "source": [
    "#### 2）Nodes（application steps）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4193c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571992e",
   "metadata": {},
   "source": [
    "#### 3）Control flow\n",
    "\n",
    "`_start_ --> retrieve --> generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d259c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d42a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex or hard task into smaller, simpler, and more manageable steps or subgoals. This can be achieved by Large Language Models (LLMs) using simple prompts or task-specific instructions, as| well as through human inputs. Techniques like Chain of Thought (CoT) instruct models to \"think step by step\" to transform big tasks into multiple manageable ones.|"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4c6c1",
   "metadata": {},
   "source": [
    "#### 不使用LangGraph实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"...\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d12d3",
   "metadata": {},
   "source": [
    "## 扩展\n",
    "\n",
    "目前，我们执行查询使用的是原始的用户输入。但用户输入可能包含不相关信息或过于低效等等。为此，我们希望能够先对用户输入进行重写，得到查询。<br>\n",
    "为上述结构增加一个流程——问句查询`query analysis`<br>\n",
    "使得流程为 ``_start_ --> query analysis --> retrieve --> generate``\n",
    "\n",
    "query analysis利用模型从用户输入中构建一个优化的搜索查询。为了达到这个目的，首先为每个splits增加一些metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7a15b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8,\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480f58c",
   "metadata": {},
   "source": [
    "由于更新了split，相应的，要更新向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7070f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02cde6",
   "metadata": {},
   "source": [
    "接下来为我们的搜索查询定义一个模式，为此，将使用结构化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39560a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., 'search query to run']\n",
    "    section: Annotated[Literal['beginning', 'middle', 'end'], ..., 'section to query']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33e3b5",
   "metadata": {},
   "source": [
    "设置控制流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa6d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19c6ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'section': 'end', 'query': 'Task Decomposition'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='07c70bd7-5b59-4c27-a867-fde8bf34fc9c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 38621, 'section': 'end'}, page_content='are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"'), Document(id='433da80f-863e-438e-ab9b-548a8d0933aa', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 34990, 'section': 'end'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",'), Document(id='f5cbf61f-516a-4670-a0bd-dbfa374f4f2b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 37831, 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'), Document(id='a0833d64-5c32-476a-b5c6-e5d054081bc9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39002, 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'I am sorry, but the provided text does not mention \"Task Decomposition\" at the end of the post or anywhere else. The end of the post discusses \"Challenges\" and \"limitations\" encountered after reviewing LLM-centered agents.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
